# ğŸ§  RAG x SLM (Small Language Model) PoC

Proyek ini menunjukkan implementasi **Retrieval-Augmented Generation (RAG)** menggunakan **Small Language Model (SLM)** seperti `phi3:mini` dari Ollama.

## ğŸš€ Fitur
- Ingest dokumen `.txt` menjadi basis pengetahuan.
- Hybrid retrieval (BM25 + dense vector).
- MMR selection untuk diversity.
- FastAPI endpoint `/ask` untuk menjawab pertanyaan berbasis konteks.
- Dukungan bahasa Indonesia & sitasi sumber.

## ğŸ“¦ Instalasi
1. Clone repositori ini atau ekstrak file ZIP.
2. Jalankan model Ollama terlebih dahulu:

```bash
ollama pull phi3:mini
ollama serve
```

3. Instal dependensi Python:

```bash
pip install -r requirements.txt
```

4. Jalankan API:

```bash
uvicorn app:app --reload --port 8000
```

## ğŸ“‚ Struktur Folder
```
rag_slm_poc/
â”œâ”€â”€ app.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ docs_txt/   # folder opsional untuk file .txt yang akan di-ingest
```

## ğŸ§© Contoh Penggunaan
### Ingest dokumen
```bash
curl -X POST http://localhost:8000/ingest_dir -H "Content-Type: application/json" -d '{"path":"./docs_txt"}'
```

### Ajukan pertanyaan
```bash
curl -X POST http://localhost:8000/ask -H "Content-Type: application/json" -d '{"question":"Apa tujuan proyek Prometix?"}'
```

## ğŸ› ï¸ Catatan
- Untuk PDF, gunakan `pdfplumber` atau `pymupdf` untuk konversi ke `.txt` terlebih dahulu.
- Parameter bisa diatur melalui environment variable:
  - `EMB_MODEL` â†’ model embedding (default: multilingual-e5-small)
  - `OLLAMA_MODEL` â†’ model Ollama (default: phi3:mini)
  - `TOP_K`, `MMR_LAMBDA` untuk tuning hasil retrieval.

---
Dibuat oleh **Kiki Ginanjar** ğŸ§‘â€ğŸ’»
